{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Smartcab to Drive Report\n",
    "## Abstract\n",
    "In this project we aim at training a **Smartcab** (may be referred to as **smart agent**) to move on a  8 by 6 grid, with randomly chosen starting point and destination. The **environment** consists of the **grid**, three other **randomly moving agents** and **traffic lights** on every intersection, switching with different time intervals.\n",
    "\n",
    "Our goal is to get our agent to the destination within the set deadline, and as fast as possible.\n",
    "\n",
    "Note that most if not all of our code mentioned in this report will be in `agent.py`. We may make minor changes to other files for logging purposes only.\n",
    "\n",
    "## Setting up the Baseline\n",
    "### Random Walk Mode\n",
    "Before we start implementing any machine learning algorithm, it is important that we know from what point we are optimizing from. And for problems like this, very often we can choose the random walk as our baseline.\n",
    "\n",
    "This can be easily implemented with one line of code (or two lines, if you count the import):\n",
    "\n",
    "```python\n",
    "import random\n",
    "action = random.choice(Environment.valid_actions)\n",
    "```\n",
    "\n",
    "where `Environment.valid_actions = [None, 'forward', 'left', 'right']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Reward Rate</th>\n",
       "      <th>Max Reward Rate</th>\n",
       "      <th>Min Reward Rate</th>\n",
       "      <th>Mode Reward Rate</th>\n",
       "      <th>Number of Penalized Trials</th>\n",
       "      <th>Number of Trials</th>\n",
       "      <th>Success Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.728</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>(0.0, 19)</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.672</td>\n",
       "      <td>3.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>(0.0, 15)</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.641</td>\n",
       "      <td>6.250</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>(0.0, 26)</td>\n",
       "      <td>97</td>\n",
       "      <td>100</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.753</td>\n",
       "      <td>10.500</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>(0.0, 24)</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.675</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>(0.0, 16)</td>\n",
       "      <td>97</td>\n",
       "      <td>100</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avg. Reward Rate  Max Reward Rate  Min Reward Rate Mode Reward Rate  \\\n",
       "0             0.728            5.000            0.038        (0.0, 19)   \n",
       "1             0.672            3.167            0.000        (0.0, 15)   \n",
       "2             0.641            6.250           -0.069        (0.0, 26)   \n",
       "3             0.753           10.500           -0.071        (0.0, 24)   \n",
       "4             0.675            4.000            0.000        (0.0, 16)   \n",
       "\n",
       "   Number of Penalized Trials  Number of Trials  Success Rate  \n",
       "0                          96               100          0.24  \n",
       "1                          98               100          0.24  \n",
       "2                          97               100          0.18  \n",
       "3                          96               100          0.16  \n",
       "4                          97               100          0.21  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "random_mode = pd.DataFrame({'Number of Trials' : 100,\n",
    "                    'Success Rate' : pd.Series([0.24, 0.24, 0.18, 0.16, 0.21]),\n",
    "                    'Number of Penalized Trials' : pd.Series([96, 98, 97, 96, 97]),\n",
    "                    'Avg. Reward Rate' : pd.Series([0.728, 0.672, 0.641, 0.753, 0.675]),\n",
    "                    'Max Reward Rate' : pd.Series([5.0, 3.167, 6.25, 10.5, 4.0]),\n",
    "                    'Min Reward Rate' : pd.Series([0.038, 0.0, -0.069, -0.071, 0.0]),\n",
    "                    'Mode Reward Rate' : pd.Series([(0.0, 19), (0.0, 15), (0.0, 26), (0.0, 24), (0.0, 16)])})\n",
    "\n",
    "display(random_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above our the data for each test run in random walk mode. The KPIs are defined as below:\n",
    "\n",
    "* Success Rate: $\\frac{\\text{Number of times the agent reaches the destination}}{\\text{Number of trials}}$\n",
    "* Number of Penalized Trials: Number of trials which get any negative reward\n",
    "* Reward Rate: $\\frac{\\text{Net reward}}{\\text{Number of steps taken to get to the destination}}$\n",
    "* Avg. Reward Rate: Mean Reward Rate of all the successful trials\n",
    "* Max Reward Rate: Maximum Reward Rate\n",
    "* Min Reward Rate: Minimum Reward Rate\n",
    "* Mode Reward Rate: Mode of Reward Rates rounded to the second digit\n",
    "\n",
    "With the KPIs set up we can see that the random walk performs rather poorly on getting our cab to its destination. And we will improve that with a better algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Against the Baseline\n",
    "### Identify the States\n",
    "Our environment assumes the US right-of-way rules. \n",
    "\n",
    "1. On a green light, you can turn left only if there is no oncoming traffic at the intersection coming straight. \n",
    "2. On a red light, you can turn right if there is no oncoming traffic turning left or traffic from the left going straight.\n",
    "\n",
    "Although violating some of the rules are not penalized for the current version, we would like take those features into account and make them into our states so that the code is ready for future change (this will of course expand our feature space and therefore we will need more trials for the agent to learn the optimal policy).\n",
    "\n",
    "Below are the inputs that our smartcab can take:\n",
    "\n",
    "1. Next waypoint\n",
    "2. Traffic light status\n",
    "3. Status of oncoming agent\n",
    "4. Status of agent on the left\n",
    "5. Status of on the right\n",
    "6. Deadline\n",
    "\n",
    "As said earlier, although the smartcab doesn't get penalized for violating the right-of-way rules (which means we don't really need to let the smartcab sense the statuses of other agents), for now we still take them into account and observe how the smart agent would learn.\n",
    "\n",
    "#### Next waypoint\n",
    "This is the essential state that we need to take into account, since the destination is different for each new trial, representing the absolute location and heading is not useful. The only way for our smartcab to ever find the destination is to follow the **next waypoint**. The smartcab gets a reward of 2 each time it correctly follows the next waypoint, and a reward 0.5 when it doesn't.\n",
    "\n",
    "#### Traffic light status\n",
    "Status of the traffic light plays an important role here as well. The reward/penalty works as follows:\n",
    "\n",
    "1. When the traffic light is 'red' and the agent goes 'forward' it gets penalized by -1\n",
    "2. When the traffic light is 'red' and the agent goes 'left' it gets penalized by -1\n",
    "3. In other cases the agent gets reward as stated in the Next waypoint section.\n",
    "\n",
    "#### Statuses of other agents\n",
    "As said ealier, statuses of other agents don't really play any roles here. We include them here so that our code is ready for future change. The feature space is slighly expanded by including them so we may need to take them out if our smart agent fails to learn the optimal policy. Yet, we may not need to.\n",
    "\n",
    "#### Deadline\n",
    "Deadline is not represented as a status explicitly for now. However, it does get implicitly represented by $\\gamma$, since the less steps it takes for the agent to get to the destination, the more valueable the reward of reaching the destination would be, which in turn makes the policy that gets to the destination faster more favorable. We may take this into account later if the performance of the smart agent is not satisfying.\n",
    "\n",
    "\n",
    "### Q-Learning and the Smarter Policy\n",
    "#### Quick and Dirty Q-Learning\n",
    "\n",
    "To start with, we implement Q-learning with epsilon-greedy algorithm to obtain policies for our smart agent. A quick and dirty implementation gives a the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Reward Rate</th>\n",
       "      <th>Max Reward Rate</th>\n",
       "      <th>Min Reward Rate</th>\n",
       "      <th>Mode Reward Rate</th>\n",
       "      <th>Number of Penalized Trials</th>\n",
       "      <th>Number of Trials</th>\n",
       "      <th>Success Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.545</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>100</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.545</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.538</td>\n",
       "      <td>99</td>\n",
       "      <td>67</td>\n",
       "      <td>100</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avg. Reward Rate  Max Reward Rate  Min Reward Rate  Mode Reward Rate  \\\n",
       "0             1.545              4.5            0.538                 0   \n",
       "1             1.545              4.5            0.538                99   \n",
       "\n",
       "   Number of Penalized Trials  Number of Trials  Success Rate  \n",
       "0                          67               100          0.46  \n",
       "1                          67               100          0.46  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qd_perf = pd.DataFrame({'Number of Trials' : 100,\n",
    "                    'Success Rate' : 0.46,\n",
    "                    'Number of Penalized Trials' : 67,\n",
    "                    'Avg. Reward Rate' : 1.545,\n",
    "                    'Max Reward Rate' : 4.5,\n",
    "                    'Min Reward Rate' : 0.538,\n",
    "                    'Mode Reward Rate' : (0.0, 99)})\n",
    "\n",
    "display(qd_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
