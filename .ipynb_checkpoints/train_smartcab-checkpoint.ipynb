{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Smartcab to Drive Report\n",
    "## Abstract\n",
    "In this project we aim at training a **Smartcab** (may be referred to as **smart agent**) to move on a  8 by 6 grid, with randomly chosen starting point and destination. The **environment** consists of the **grid**, three other **randomly moving agents** and **traffic lights** on every intersection, switching with different time intervals.\n",
    "\n",
    "Our goal is to get our agent to the destination within the set deadline, and as fast as possible.\n",
    "\n",
    "Note that most if not all of our code mentioned in this report will be in `agent.py`. We may make minor changes to other files for logging purposes only.\n",
    "\n",
    "## Setting up the Baseline\n",
    "### Random Walk Mode\n",
    "Before we start implementing any machine learning algorithm, it is important that we know from what point we are optimizing from. And for problems like this, very often we can choose the random walk as our baseline.\n",
    "\n",
    "This can be easily implemented with one line of code (or two lines, if you count the import):\n",
    "\n",
    "```python\n",
    "import random\n",
    "action = random.choice(Environment.valid_actions)\n",
    "```\n",
    "\n",
    "where `Environment.valid_actions = [None, 'forward', 'left', 'right']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Reward Rate</th>\n",
       "      <th>Max Reward Rate</th>\n",
       "      <th>Min Reward Rate</th>\n",
       "      <th>Mode Reward Rate</th>\n",
       "      <th>Number of Penalized Trials</th>\n",
       "      <th>Number of Trials</th>\n",
       "      <th>Success Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.728</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>(0.0, 19)</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.672</td>\n",
       "      <td>3.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>(0.0, 15)</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.641</td>\n",
       "      <td>6.250</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>(0.0, 26)</td>\n",
       "      <td>97</td>\n",
       "      <td>100</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.753</td>\n",
       "      <td>10.500</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>(0.0, 24)</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.675</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>(0.0, 16)</td>\n",
       "      <td>97</td>\n",
       "      <td>100</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avg. Reward Rate  Max Reward Rate  Min Reward Rate Mode Reward Rate  \\\n",
       "0             0.728            5.000            0.038        (0.0, 19)   \n",
       "1             0.672            3.167            0.000        (0.0, 15)   \n",
       "2             0.641            6.250           -0.069        (0.0, 26)   \n",
       "3             0.753           10.500           -0.071        (0.0, 24)   \n",
       "4             0.675            4.000            0.000        (0.0, 16)   \n",
       "\n",
       "   Number of Penalized Trials  Number of Trials  Success Rate  \n",
       "0                          96               100          0.24  \n",
       "1                          98               100          0.24  \n",
       "2                          97               100          0.18  \n",
       "3                          96               100          0.16  \n",
       "4                          97               100          0.21  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "random_mode = pd.DataFrame({'Number of Trials' : 100,\n",
    "                    'Success Rate' : pd.Series([0.24, 0.24, 0.18, 0.16, 0.21]),\n",
    "                    'Number of Penalized Trials' : pd.Series([96, 98, 97, 96, 97]),\n",
    "                    'Avg. Reward Rate' : pd.Series([0.728, 0.672, 0.641, 0.753, 0.675]),\n",
    "                    'Max Reward Rate' : pd.Series([5.0, 3.167, 6.25, 10.5, 4.0]),\n",
    "                    'Min Reward Rate' : pd.Series([0.038, 0.0, -0.069, -0.071, 0.0]),\n",
    "                    'Mode Reward Rate' : pd.Series([(0.0, 19), (0.0, 15), (0.0, 26), (0.0, 24), (0.0, 16)])})\n",
    "\n",
    "display(random_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above our the data for each test run in random walk mode. The KPIs are defined as below:\n",
    "\n",
    "* **Success Rate:** $\\frac{\\text{Number of times the agent reaches the destination}}{\\text{Number of trials}}$\n",
    "* **Number of Penalized Trials:** Number of trials which get any negative reward\n",
    "* **Reward Rate:** $\\frac{\\text{Net reward}}{\\text{Number of steps taken to get to the destination}}$\n",
    "* **Avg. Reward Rate:** Mean Reward Rate of all the successful trials\n",
    "* **Max Reward Rate:** Maximum Reward Rate\n",
    "* **Min Reward Rate:** Minimum Reward Rate\n",
    "* **Mode Reward Rate:** Mode of Reward Rates rounded to the first decimal point\n",
    "\n",
    "Since our goal is to get to the destination as fast as possible while getting as much reward as possible (following traffic rules and not getting penalized), the most important factor here is the **Reward Rate** (the higher the better). This, however, doesn't tell us everything. Our smartcab may learn to \"be smart\" and try to get to the destination fast at all cost, breaking the traffic rules as a tradeoff. So in addition to this, we also want to monitor the **Number of Penalized Trials** (the lower the better) to make sure our smartcab learns to follow the traffic rules.\n",
    "\n",
    "In addition to this, we also want our smartcab to learn fast, and get more high Reward Rates during the 100 trials. This makes the primary KPIs **Mode** and **Average** of the Reward Rate (we would like both the value and the occurence high for mode and the value high for average). We can also look at the **Success Rate** which monitors the same behavior but is defined to give us a more macroscopic view.\n",
    "\n",
    "With the KPIs set up we can see that the random walk performs rather poorly on getting our cab to its destination. And we will improve that with a better algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Against the Baseline\n",
    "### Identify the States\n",
    "Our environment assumes the US right-of-way rules. \n",
    "\n",
    "1. On a green light, you can turn left only if there is no oncoming traffic at the intersection coming straight. \n",
    "2. On a red light, you can turn right if there is no oncoming traffic turning left or traffic from the left going straight.\n",
    "\n",
    "Although violating some of the rules are not penalized for the current version, we would like take those features into account and make them into our states so that the code is ready for future change (this will of course expand our feature space and therefore we will need more trials for the agent to learn the optimal policy).\n",
    "\n",
    "Below are the inputs that our smartcab can take:\n",
    "\n",
    "1. Next waypoint\n",
    "2. Traffic light status\n",
    "3. Status of oncoming agent\n",
    "4. Status of agent on the left\n",
    "5. Status of on the right\n",
    "6. Deadline\n",
    "\n",
    "As said earlier, although the smartcab doesn't get penalized for violating the right-of-way rules (which means we don't really need to let the smartcab sense the statuses of other agents), for now we still take them into account and observe how the smart agent would learn.\n",
    "\n",
    "#### Next waypoint\n",
    "This is the essential state that we need to take into account, since the destination is different for each new trial, representing the absolute location and heading is not useful. The only way for our smartcab to ever find the destination is to follow the **next waypoint**. The smartcab gets a reward of 2 each time it correctly follows the next waypoint, and a reward 0.5 when it doesn't.\n",
    "\n",
    "#### Traffic light status\n",
    "Status of the traffic light plays an important role here as well. The reward/penalty works as follows:\n",
    "\n",
    "1. When the traffic light is 'red' and the agent goes 'forward' it gets penalized by -1\n",
    "2. When the traffic light is 'red' and the agent goes 'left' it gets penalized by -1\n",
    "3. In other cases the agent gets reward as stated in the Next waypoint section.\n",
    "\n",
    "#### Statuses of other agents\n",
    "As said ealier, statuses of other agents don't really play any roles here. We include them here so that our code is ready for future change. The feature space is slighly expanded by including them so we may need to take them out if our smart agent fails to learn the optimal policy. Yet, we may not need to.\n",
    "\n",
    "#### Deadline\n",
    "Deadline is not represented as a status explicitly for now. However, it does get implicitly represented by $\\gamma$, since the less steps it takes for the agent to get to the destination, the more valueable the reward of reaching the destination would be, which in turn makes the policy that gets to the destination faster more favorable. We may take this into account later if the performance of the smart agent is not satisfying.\n",
    "\n",
    "\n",
    "### Q-Learning and the Smarter Policy\n",
    "#### Quick and dirty Q-learning\n",
    "\n",
    "To start with, we implement Q-learning with epsilon-greedy algorithm to obtain policies for our smart agent. A quick and dirty implementation gives a the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Reward Rate</th>\n",
       "      <th>Max Reward Rate</th>\n",
       "      <th>Min Reward Rate</th>\n",
       "      <th>Mode Reward Rate</th>\n",
       "      <th>Number of Penalized Trials</th>\n",
       "      <th>Number of Trials</th>\n",
       "      <th>Success Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.06</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.558</td>\n",
       "      <td>(2.2, 9)</td>\n",
       "      <td>54</td>\n",
       "      <td>100</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avg. Reward Rate  Max Reward Rate  Min Reward Rate Mode Reward Rate  \\\n",
       "0              2.06              5.5            0.558         (2.2, 9)   \n",
       "\n",
       "   Number of Penalized Trials  Number of Trials  Success Rate  \n",
       "0                          54               100          0.76  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qd_perf = pd.DataFrame({'Number of Trials' : 100,\n",
    "                    'Success Rate' : 0.76,\n",
    "                    'Number of Penalized Trials' : 54,\n",
    "                    'Avg. Reward Rate' : 2.06,\n",
    "                    'Max Reward Rate' : 5.5,\n",
    "                    'Min Reward Rate' : 0.558,\n",
    "                    'Mode Reward Rate': [(2.2, 9)]})\n",
    "\n",
    "display(qd_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is huge improvement! In this quick dirty version of Q-learning we start with complete randomness and decrease the probability of going random by 1% (which means we increase the probability of following the current best policy learned with Q-learning) each time. The decrease stop at when the probablity of going random is at 20%.\n",
    "\n",
    "We won't address all of other factors here, such as the learning rate $\\alpha$ or the discount factor $\\gamma$. Also since the driving policy is very much random so we can't say that we can get constant good results like this, but nonetheless this is a good start.\n",
    "\n",
    "#### Training-Testing\n",
    "Our goal is to learn a feasible policy within 100 trials. And to test how well the agent has learned, we let it run another about 400 trials following the learned policy and inspect the statistics (we decrease the probability of going random by $\\frac{1}{100}$ for each trial until it reaches 0. So from Trial 100 our agent would start completely following the policy it's learned from the previous 100 trials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Distance</th>\n",
       "      <th>Avg. Reward Rate</th>\n",
       "      <th>Avg. Steps</th>\n",
       "      <th>D/S</th>\n",
       "      <th>Fail Trials</th>\n",
       "      <th>Mode Reward Rate</th>\n",
       "      <th>Penalized Trials</th>\n",
       "      <th>Penalty Rate</th>\n",
       "      <th>SD Reward Rate</th>\n",
       "      <th>Success Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.388</td>\n",
       "      <td>2.702</td>\n",
       "      <td>14.635</td>\n",
       "      <td>0.3</td>\n",
       "      <td>(107, 108, 114, 128, 146, 149, 162, 169, 170, ...</td>\n",
       "      <td>(2.1, 36)</td>\n",
       "      <td>(106, 117, 145, 149, 162, 175, 191, 214, 220, ...</td>\n",
       "      <td>0.062344</td>\n",
       "      <td>1.112</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avg. Distance  Avg. Reward Rate  Avg. Steps  D/S  \\\n",
       "0          4.388             2.702      14.635  0.3   \n",
       "\n",
       "                                         Fail Trials Mode Reward Rate  \\\n",
       "0  (107, 108, 114, 128, 146, 149, 162, 169, 170, ...        (2.1, 36)   \n",
       "\n",
       "                                    Penalized Trials  Penalty Rate  \\\n",
       "0  (106, 117, 145, 149, 162, 175, 191, 214, 220, ...      0.062344   \n",
       "\n",
       "   SD Reward Rate  Success Rate  \n",
       "0           1.112         0.913  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_perf = pd.DataFrame({\n",
    "                    'Success Rate': 0.913,\n",
    "                    'Fail Trials': [(107, 108, 114, 128, 146, 149, 162, 169, 170, 221, 260, 263, 272, 275, 281, 294, 295, 318, 328, 331, 350, 353, 362, 366, 405, 406, 409, 425, 428, 431, 452, 477, 486, 487, 494)],\n",
    "                    'Avg. Distance': 4.388,\n",
    "                    'Avg. Steps': 14.635,\n",
    "                    'D/S': 0.300,   \n",
    "                    'Penalty Rate': 25 / 401,\n",
    "                    'Penalized Trials': [(106, 117, 145, 149, 162, 175, 191, 214, 220, 222, 226, 260, 286, 303, 307, 320, 353, 354, 362, 390, 393, 401, 426, 456, 465)],\n",
    "                    'Avg. Reward Rate' : 2.702,\n",
    "                    'Mode Reward Rate': [(2.1, 36)],\n",
    "                    'SD Reward Rate': 1.112})\n",
    "\n",
    "display(first_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note here is that here we have made some minor changes to our KPIs, where we only get our statistics from our testing trials (rounds that strictly follow the policy the agent's learned). We have also added average steps needed to get a better understanding of how efficient our agent is. SD is also present to help us understand the consistency.\n",
    "\n",
    "#### KPI review\n",
    "It appears that we are not so far away from our optimal policy now, give the Success Rate at 91.3% and Penalty Rate at 6.23%. Let's again review what KPIs we have and decide what we should do next.\n",
    "\n",
    "##### Primary KPIs\n",
    "The most important KPIs are of course:\n",
    "\n",
    "* Success Rate\n",
    "* Penalized Trials\n",
    "\n",
    "If the smartcab doesn't even get to the destianation, then it is not really that smart and it doesn't really matter how much reward it's collected. Also out of our 400 test-drive trials, about 6% of trials still get penalized.\n",
    "\n",
    "Let's address these issues.\n",
    "\n",
    "[107, 108, 114, 128, 146, 149, 162, 169, 170, 221, 260, 263, 272, 275, 281, 294, 295, 318, 328, 331, 350, 353, 362, 366, 405, 406, 409, 425, 428, 431, 452, 477, 486, 487, 494]\n",
    "\n",
    "Simulator.run(): Trial 494\n",
    "Environment.reset(): Trial set up with start = (1, 1), destination = (8, 2), deadline = 40\n",
    "RoutePlanner.route_to(): destination = (8, 2)\n",
    "40 Deadlines\n",
    "24 Red lights\n",
    "16 None Actions\n",
    "\n",
    "Simulator.run(): Trial 487\n",
    "Environment.reset(): Trial set up with start = (1, 4), destination = (5, 6), deadline = 30\n",
    "RoutePlanner.route_to(): destination = (5, 6)\n",
    "30 Deadlines\n",
    "16 Red Lights\n",
    "5 None Actions\n",
    "\n",
    "Simulator.run(): Trial 353\n",
    "Environment.reset(): Trial set up with start = (5, 2), destination = (1, 6), deadline = 40\n",
    "RoutePlanner.route_to(): destination = (1, 6)\n",
    "40 Dead Lines\n",
    "21 Red Lights\n",
    "11 None Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Fiddle with $\\gamma$\n",
    "$\\gamma$ as the discount factor can be seen as an indicator of how valueable we think a future value is. By decreasing $\\gamma$ we can encourage our agent to get to the destiantion even faster. There's a limit though to how much we can decrease $\\gamma$. If we depreciate the future value too much, then at some point it makes no real difference for our agent to either get there or not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
