{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Smartcab to Drive Report\n",
    "## Abstract\n",
    "In this project we aim at training a **Smartcab** (may be referred to as **smart agent**) to move on a  8 by 6 grid, with randomly chosen starting point and destination. The **environment** consists of the **grid**, three other **randomly moving agents** and **traffic lights** on every intersection, switching with different time intervals.\n",
    "\n",
    "Our goal is to get our agent to the destination within the set deadline, and as fast as possible.\n",
    "\n",
    "Note that most if not all of our code mentioned in this report will be in `agent.py`. We may make minor changes to other files for logging purposes only.\n",
    "\n",
    "## Setting up the Baseline\n",
    "### Random Walk Mode\n",
    "Before we start implementing any machine learning algorithm, it is important that we know from what point we are optimizing from. And for problems like this, very often we can choose the random walk as our baseline.\n",
    "\n",
    "This can be easily implemented with one line of code (or two lines, if you count the import):\n",
    "\n",
    "```python\n",
    "import random\n",
    "action = random.choice(Environment.valid_actions)\n",
    "```\n",
    "\n",
    "where `Environment.valid_actions = [None, 'forward', 'left', 'right']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Reward Rate</th>\n",
       "      <th>Max Reward Rate</th>\n",
       "      <th>Min Reward Rate</th>\n",
       "      <th>Mode Reward Rate</th>\n",
       "      <th>Number of Penalized Trials</th>\n",
       "      <th>Number of Trials</th>\n",
       "      <th>Success Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.728</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.038</td>\n",
       "      <td>(0.0, 19)</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.672</td>\n",
       "      <td>3.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>(0.0, 15)</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.641</td>\n",
       "      <td>6.250</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>(0.0, 26)</td>\n",
       "      <td>97</td>\n",
       "      <td>100</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.753</td>\n",
       "      <td>10.500</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>(0.0, 24)</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.675</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>(0.0, 16)</td>\n",
       "      <td>97</td>\n",
       "      <td>100</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avg. Reward Rate  Max Reward Rate  Min Reward Rate Mode Reward Rate  \\\n",
       "0             0.728            5.000            0.038        (0.0, 19)   \n",
       "1             0.672            3.167            0.000        (0.0, 15)   \n",
       "2             0.641            6.250           -0.069        (0.0, 26)   \n",
       "3             0.753           10.500           -0.071        (0.0, 24)   \n",
       "4             0.675            4.000            0.000        (0.0, 16)   \n",
       "\n",
       "   Number of Penalized Trials  Number of Trials  Success Rate  \n",
       "0                          96               100          0.24  \n",
       "1                          98               100          0.24  \n",
       "2                          97               100          0.18  \n",
       "3                          96               100          0.16  \n",
       "4                          97               100          0.21  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "random_mode = pd.DataFrame({'Number of Trials' : 100,\n",
    "                    'Success Rate' : pd.Series([0.24, 0.24, 0.18, 0.16, 0.21]),\n",
    "                    'Number of Penalized Trials' : pd.Series([96, 98, 97, 96, 97]),\n",
    "                    'Avg. Reward Rate' : pd.Series([0.728, 0.672, 0.641, 0.753, 0.675]),\n",
    "                    'Max Reward Rate' : pd.Series([5.0, 3.167, 6.25, 10.5, 4.0]),\n",
    "                    'Min Reward Rate' : pd.Series([0.038, 0.0, -0.069, -0.071, 0.0]),\n",
    "                    'Mode Reward Rate' : pd.Series([(0.0, 19), (0.0, 15), (0.0, 26), (0.0, 24), (0.0, 16)])})\n",
    "\n",
    "display(random_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above our the data for each test run in random walk mode. The KPIs are defined as below:\n",
    "\n",
    "* **Success Rate:** $\\frac{\\text{Number of times the agent reaches the destination}}{\\text{Number of trials}}$\n",
    "* **Number of Penalized Trials:** Number of trials which get any negative reward\n",
    "* **Reward Rate:** $\\frac{\\text{Net reward}}{\\text{Number of steps taken to get to the destination}}$\n",
    "* **Avg. Reward Rate:** Mean Reward Rate of all the successful trials\n",
    "* **Max Reward Rate:** Maximum Reward Rate\n",
    "* **Min Reward Rate:** Minimum Reward Rate\n",
    "* **Mode Reward Rate:** Mode of Reward Rates rounded to the first decimal point\n",
    "\n",
    "Since our goal is to get to the destination as fast as possible while getting as much reward as possible (following traffic rules and not getting penalized), the most important factor here is the **Reward Rate** (the higher the better). This, however, doesn't tell us everything. Our smartcab may learn to \"be smart\" and try to get to the destination fast at all cost, breaking the traffic rules as a tradeoff. So in addition to this, we also want to monitor the **Number of Penalized Trials** (the lower the better) to make sure our smartcab learns to follow the traffic rules.\n",
    "\n",
    "In addition to this, we also want our smartcab to learn fast, and get more high Reward Rates during the 100 trials. This makes the primary KPIs **Mode** and **Average** of the Reward Rate (we would like both the value and the occurence high for mode and the value high for average). We can also look at the **Success Rate** which monitors the same behavior but is defined to give us a more macroscopic view.\n",
    "\n",
    "With the KPIs set up we can see that the random walk performs rather poorly on getting our cab to its destination. And we will improve that with a better algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Against the Baseline\n",
    "### Identify the States\n",
    "Our environment assumes the US right-of-way rules. \n",
    "\n",
    "1. On a green light, you can turn left only if there is no oncoming traffic at the intersection coming straight. \n",
    "2. On a red light, you can turn right if there is no oncoming traffic turning left or traffic from the left going straight.\n",
    "\n",
    "Although violating some of the rules are not penalized for the current version, we would like take those features into account and make them into our states so that the code is ready for future change (this will of course expand our feature space and therefore we will need more trials for the agent to learn the optimal policy).\n",
    "\n",
    "Below are the inputs that our smartcab can take:\n",
    "\n",
    "1. Next waypoint\n",
    "2. Traffic light status\n",
    "3. Status of oncoming agent\n",
    "4. Status of agent on the left\n",
    "5. Status of on the right\n",
    "6. Deadline\n",
    "\n",
    "As said earlier, although the smartcab doesn't get penalized for violating the right-of-way rules (which means we don't really need to let the smartcab sense the statuses of other agents), for now we still take them into account and observe how the smart agent would learn.\n",
    "\n",
    "#### Next waypoint\n",
    "This is the essential state that we need to take into account, since the destination is different for each new trial, representing the absolute location and heading is not useful. The only way for our smartcab to ever find the destination is to follow the **next waypoint**. The smartcab gets a reward of 2 each time it correctly follows the next waypoint, and a reward 0.5 when it doesn't.\n",
    "\n",
    "#### Traffic light status\n",
    "Status of the traffic light plays an important role here as well. The reward/penalty works as follows:\n",
    "\n",
    "1. When the traffic light is 'red' and the agent goes 'forward' it gets penalized by -1\n",
    "2. When the traffic light is 'red' and the agent goes 'left' it gets penalized by -1\n",
    "3. In other cases the agent gets reward as stated in the Next waypoint section.\n",
    "\n",
    "#### Statuses of other agents\n",
    "As said ealier, statuses of other agents don't really play any roles here. We include them here so that our code is ready for future change. The feature space is slighly expanded by including them so we may need to take them out if our smart agent fails to learn the optimal policy. Yet, we may not need to.\n",
    "\n",
    "#### Deadline\n",
    "Deadline is not represented as a status explicitly for now. However, it does get implicitly represented by $\\gamma$, since the less steps it takes for the agent to get to the destination, the more valueable the reward of reaching the destination would be, which in turn makes the policy that gets to the destination faster more favorable. We may take this into account later if the performance of the smart agent is not satisfying.\n",
    "\n",
    "\n",
    "### Q-Learning and the Smarter Policy\n",
    "#### Quick and dirty Q-learning\n",
    "\n",
    "To start with, we implement Q-learning with epsilon-greedy algorithm to obtain policies for our smart agent. A quick and dirty implementation gives a the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Reward Rate</th>\n",
       "      <th>Max Reward Rate</th>\n",
       "      <th>Min Reward Rate</th>\n",
       "      <th>Mode Reward Rate</th>\n",
       "      <th>Number of Penalized Trials</th>\n",
       "      <th>Number of Trials</th>\n",
       "      <th>Success Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.06</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.558</td>\n",
       "      <td>(2.2, 9)</td>\n",
       "      <td>54</td>\n",
       "      <td>100</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avg. Reward Rate  Max Reward Rate  Min Reward Rate Mode Reward Rate  \\\n",
       "0              2.06              5.5            0.558         (2.2, 9)   \n",
       "\n",
       "   Number of Penalized Trials  Number of Trials  Success Rate  \n",
       "0                          54               100          0.76  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qd_perf = pd.DataFrame({'Number of Trials' : 100,\n",
    "                    'Success Rate' : 0.76,\n",
    "                    'Number of Penalized Trials' : 54,\n",
    "                    'Avg. Reward Rate' : 2.06,\n",
    "                    'Max Reward Rate' : 5.5,\n",
    "                    'Min Reward Rate' : 0.558,\n",
    "                    'Mode Reward Rate': [(2.2, 9)]})\n",
    "\n",
    "display(qd_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is huge improvement! In this quick dirty version of Q-learning we start with complete randomness and decrease the probability of going random by 1% (which means we increase the probability of following the current best policy learned with Q-learning) each time. The decrease stop at when the probablity of going random is at 20%.\n",
    "\n",
    "We won't address all of other factors here, such as the learning rate $\\alpha$ or the discount factor. Also since the driving policy is very much random so we can't say that we can get constant good results like this, but nonetheless this is a good start.\n",
    "\n",
    "#### Tweaking the parameters\n",
    "Before we start tweaking parameters for our Q-learning algorithm, we need to first decide how much training data is necessary to train our smartcab.\n",
    "\n",
    "Running our code a few time we can see that at Random Rounds = 60 (we decrease the probability of going random by $\\frac{1}{6}$ for each trial until it reaches 0. So from Trial 60 our agent would start completely following the policy it's learned from the previous 60 trials), we can get rather consistent results with small standard deviation for the Reward Rates, and mean and mode being really close.\n",
    "\n",
    "Running 3 times with Random Rounds = 60, we get averages of:\n",
    "\n",
    "* Success Rate: 0.967\n",
    "* Average Steps Needed: 13.525\n",
    "* Number of Penalized Trials: 2\n",
    "* Average Reward Rate: 2.440\n",
    "* Mode of Reward Rates: 2.233\n",
    "* SD of Reward Rates: 0.574\n",
    "\n",
    "Apparently we could increase the number of random rounds to get ourselves more \"unseen\" data to increase the performance. However it seems 60 is the Goldilocks amount that serves our purpose all right.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
